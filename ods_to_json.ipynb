{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ad47074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON files created: nodes.json, links.json, cbblinks.json, objectives.json, artifacts.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# --- CONFIG ---\n",
    "input_file = \"nodes_and_links.ods\"\n",
    "output_dir = \".\"   # can be changed if you want to save elsewhere\n",
    "\n",
    "# --- READ SHEETS ---\n",
    "nodes_df = pd.read_excel(input_file, sheet_name=\"Nodes\", engine=\"odf\")\n",
    "links_df = pd.read_excel(input_file, sheet_name=\"Links\", engine=\"odf\")\n",
    "cbbs_df  = pd.read_excel(input_file, sheet_name=\"CBB_list\", engine=\"odf\")\n",
    "objectives_df = pd.read_excel(input_file, sheet_name=\"Objectives\", engine=\"odf\")\n",
    "artifacts_df = pd.read_excel(input_file, sheet_name=\"Artifacts\", engine=\"odf\")\n",
    "\n",
    "# --- CLEAN BASIC STRUCTURE ---\n",
    "nodes_df = nodes_df.fillna(\"\")\n",
    "links_df = links_df.fillna(\"\")\n",
    "cbbs_df  = cbbs_df.fillna(\"\")\n",
    "objectives_df = objectives_df.fillna(\"\")\n",
    "artifacts_df = artifacts_df.fillna(\"\")\n",
    "\n",
    "# --- CONVERT TO DICTIONARIES ---\n",
    "nodes_json = nodes_df.to_dict(orient=\"records\")\n",
    "links_json = links_df.to_dict(orient=\"records\")\n",
    "\n",
    "# --- CBB STRUCTURE ---\n",
    "# Combine cbbs and definitions per id\n",
    "cbbs_grouped = (\n",
    "    cbbs_df.groupby(\"id\")\n",
    "    .apply(lambda x: [\n",
    "        {\"cbb\": c, \"definition\": d} \n",
    "        for c, d in zip(x[\"cbbs\"], x[\"definitions\"])\n",
    "    ])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "cbbs_json = [{\"id\": node_id, \"cbbs\": items} for node_id, items in cbbs_grouped.items()]\n",
    "\n",
    "\n",
    "def process_objectives(df):\n",
    "    \"\"\"Convert a sheet with id + multiline text columns into a JSON-friendly format.\"\"\"\n",
    "    df = df.dropna(subset=[\"id\"])  # Remove blank rows\n",
    "    df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Split text on newlines or semicolons\n",
    "        items = []\n",
    "        if pd.notna(row.iloc[1]):  # Assuming the second column has the data\n",
    "            text = str(row.iloc[1]).strip()\n",
    "            # Split on newline or semicolon\n",
    "            for item in [x.strip() for x in text.replace(\"\\r\", \"\").split(\"\\n\") if x.strip()]:\n",
    "                items.append(item)\n",
    "        records.append({\"id\": row[\"id\"], \"objectives\": items})\n",
    "\n",
    "    return records\n",
    "\n",
    "def process_artifacts(df):\n",
    "    \"\"\"Convert a sheet with id + multiline text columns into a JSON-friendly format.\"\"\"\n",
    "    df = df.dropna(subset=[\"id\"])  # Remove blank rows\n",
    "    df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Split text on newlines or semicolons\n",
    "        items = []\n",
    "        if pd.notna(row.iloc[1]):  # Assuming the second column has the data\n",
    "            text = str(row.iloc[1]).strip()\n",
    "            # Split on newline or semicolon\n",
    "            for item in [x.strip() for x in text.replace(\"\\r\", \"\").split(\"\\n\") if x.strip()]:\n",
    "                items.append(item)\n",
    "        records.append({\"id\": row[\"id\"], \"artifacts\": items})\n",
    "\n",
    "    return records\n",
    "\n",
    "# Process the two sheets\n",
    "objectives_json = process_objectives(objectives_df)\n",
    "artifacts_json = process_artifacts(artifacts_df)\n",
    "\n",
    "# --- SAVE TO JSON FILES ---\n",
    "with open(f\"{output_dir}/nodes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(nodes_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(f\"{output_dir}/links.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(links_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(f\"{output_dir}/cbblinks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cbbs_json, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "with open(f\"{output_dir}/objectives.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(objectives_json, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "with open(f\"{output_dir}/artifacts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(artifacts_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ JSON files created: nodes.json, links.json, cbblinks.json, objectives.json, artifacts.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cabf640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Grouped objectives.json written successfully.\n",
      "üì¶ Total unique IDs: 35\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# --- Load your existing objectives.json ---\n",
    "with open(\"objectives.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Group by id ---\n",
    "grouped = defaultdict(list)\n",
    "for entry in data:\n",
    "    grouped[str(entry[\"id\"])].extend(entry[\"objectives\"])\n",
    "\n",
    "# --- Convert to the correct format ---\n",
    "result = [{\"id\": k, \"objectives\": v} for k, v in grouped.items()]\n",
    "\n",
    "# --- Save back to a new JSON file ---\n",
    "with open(\"objectives_grouped.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Grouped objectives.json written successfully.\")\n",
    "print(f\"üì¶ Total unique IDs: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6050d06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Grouped artifacts.json written successfully.\n",
      "üì¶ Total unique IDs: 35\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# --- Load your existing objectives.json ---\n",
    "with open(\"artifacts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Group by id ---\n",
    "grouped = defaultdict(list)\n",
    "for entry in data:\n",
    "    grouped[str(entry[\"id\"])].extend(entry[\"artifacts\"])\n",
    "\n",
    "# --- Convert to the correct format ---\n",
    "result = [{\"id\": k, \"artifacts\": v} for k, v in grouped.items()]\n",
    "\n",
    "# --- Save back to a new JSON file ---\n",
    "with open(\"artifacts_grouped.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Grouped artifacts.json written successfully.\")\n",
    "print(f\"üì¶ Total unique IDs: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b3dc97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Found 7 JSON file(s) in /home/violet/Documents/projects/it_cmf/database_workspace ‚Äî moving to /home/violet/Documents/projects/it_cmf/html_workspace\n",
      "‚úÖ Moved nodes.json ‚Üí /home/violet/Documents/projects/it_cmf/html_workspace/nodes.json\n",
      "‚úÖ Moved objectives.json ‚Üí /home/violet/Documents/projects/it_cmf/html_workspace/objectives.json\n",
      "‚úÖ Moved artifacts.json ‚Üí /home/violet/Documents/projects/it_cmf/html_workspace/artifacts.json\n",
      "‚úÖ Moved artifacts_grouped.json ‚Üí /home/violet/Documents/projects/it_cmf/html_workspace/artifacts_grouped.json\n",
      "‚úÖ Moved cbblinks.json ‚Üí /home/violet/Documents/projects/it_cmf/html_workspace/cbblinks.json\n",
      "‚úÖ Moved objectives_grouped.json ‚Üí /home/violet/Documents/projects/it_cmf/html_workspace/objectives_grouped.json\n",
      "‚úÖ Moved links.json ‚Üí /home/violet/Documents/projects/it_cmf/html_workspace/links.json\n",
      "üéâ Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def move_json_files(destination_relative=\"../html_workspace\", overwrite=True):\n",
    "    \"\"\"\n",
    "    Move all .json files from the script directory (source) into destination_relative\n",
    "    (which is interpreted relative to the script directory). Works in interactive\n",
    "    interpreters too (falls back to current working directory).\n",
    "    \"\"\"\n",
    "\n",
    "    # Resolve source folder robustly:\n",
    "    if \"__file__\" in globals():\n",
    "        # When script is run as a file\n",
    "        source_folder = Path(__file__).resolve().parent\n",
    "    else:\n",
    "        # When running in interactive session / notebook\n",
    "        source_folder = Path.cwd()\n",
    "\n",
    "    # Resolve destination relative to the source folder\n",
    "    destination_folder = (source_folder / destination_relative).resolve()\n",
    "\n",
    "    # Ensure destination exists\n",
    "    destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Find JSON files in source folder\n",
    "    json_files = [p for p in source_folder.iterdir() if p.is_file() and p.suffix.lower() == \".json\"]\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"‚ö†Ô∏è No JSON files found in {source_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üì¶ Found {len(json_files)} JSON file(s) in {source_folder} ‚Äî moving to {destination_folder}\")\n",
    "\n",
    "    for src_path in json_files:\n",
    "        dest_path = destination_folder / src_path.name\n",
    "\n",
    "        if dest_path.exists() and not overwrite:\n",
    "            base = src_path.stem\n",
    "            ext = src_path.suffix\n",
    "            counter = 1\n",
    "            while (destination_folder / f\"{base}_{counter}{ext}\").exists():\n",
    "                counter += 1\n",
    "            dest_path = destination_folder / f\"{base}_{counter}{ext}\"\n",
    "\n",
    "        shutil.move(str(src_path), str(dest_path))\n",
    "        print(f\"‚úÖ Moved {src_path.name} ‚Üí {dest_path}\")\n",
    "\n",
    "    print(\"üéâ Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # default destination_relative is \"../folder_2\" (one level up into folder_2)\n",
    "    # you can change it, e.g. move_json_files(destination_relative=\"../../data/folder2\")\n",
    "    move_json_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd44cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
